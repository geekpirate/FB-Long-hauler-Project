{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMt2pADKn1Rkvt55GWCtaL6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekpirate/FB-Long-hauler-Project/blob/main/symptomDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm8neVB9oDVF",
        "outputId": "1391e090-662b-4383-f40b-b0fd96f0679f"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from idna import unichr\n",
        "from joblib.numpy_pickle_utils import xrange\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import io\n",
        "import nltk\n",
        "import csv, os\n",
        "import unicodedata, re , sys\n",
        "from textblob import TextBlob\n",
        "# uncomment below lines when the code is compiled for the first time\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6qyiK9y0rNa"
      },
      "source": [
        "sent = \"I had covid in September.  I spent 9 days in the hospital and 2 months at home before going back to work.  I've had so many different symptoms.  I have a new one. My breast really hurt. Both sides hurt and are hard to the touch. I will be making an appointment with my doctor. Has anyone else had this happen to them.  I truly want to feel better again.\"\n",
        "#sent = \"Is there any pregnant women in the group that tested positive for Covid ? How long did the symptoms last ? I am in day 8 of symptoms . Went to the ER 2 days ago because of my chest pressure which I know is common with Covid & I have pneumonia in my bottom right lung so currently taking an antibiotic for that .\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw8f0Ubs0tPU"
      },
      "source": [
        "lemmatizer=WordNetLemmatizer()\n",
        "s = set(stopwords.words('english'))\n",
        "text_tokens = word_tokenize(sent)\n",
        "out = [word for word in text_tokens if not word in s]\n",
        "out = [word for word in out if not word in string.punctuation]\n",
        "#out = [lemmatizer.lemmatize(word) for word in out]\n",
        "out = [word for word in out if not (nltk.pos_tag(word)[0][1] == \"PRP\")]\n",
        "#out = [word for word in out if (nltk.pos_tag(word)[0][1] == \"NN\")]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEbcQMz7y2C7"
      },
      "source": [
        "def basic_clean(text):\n",
        "  \"\"\"\n",
        "  A simple function to clean up the data. All the words that\n",
        "  are not designated as a stop word is then lemmatized after\n",
        "  encoding and basic regex parsing are performed.\n",
        "  \"\"\"\n",
        "  wnl = nltk.stem.WordNetLemmatizer()\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  text = (unicodedata.normalize('NFKD', text)\n",
        "    .encode('ascii', 'ignore')\n",
        "    .decode('utf-8', 'ignore')\n",
        "    .lower())\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  words = [word for word in words if not word.isdigit()]\n",
        "  return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
        "final = basic_clean(sent)\n",
        "\n",
        "final1 = basic_clean(sent1)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq5pE4hRz2-c",
        "outputId": "974eb942-00a0-4745-d90e-2e34724012b4"
      },
      "source": [
        "import pandas as pd\n",
        "last = [pd.Series(nltk.ngrams(out, 2))]\n",
        "# print(last)\n",
        "# print(len(out))\n",
        "print(out)\n",
        "print(nltk.pos_tag(out))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['covid', 'September', 'spent', '9', 'days', 'hospital', '2', 'months', 'home', 'going', 'back', 'work', \"'ve\", 'many', 'different', 'symptoms', 'new', 'one', 'My', 'breast', 'really', 'hurt', 'Both', 'sides', 'hurt', 'hard', 'touch', 'making', 'appointment', 'doctor', 'Has', 'anyone', 'else', 'happen', 'truly', 'want', 'feel', 'better']\n",
            "[('covid', 'JJ'), ('September', 'NNP'), ('spent', 'VBD'), ('9', 'CD'), ('days', 'NNS'), ('hospital', 'JJ'), ('2', 'CD'), ('months', 'NNS'), ('home', 'NN'), ('going', 'VBG'), ('back', 'RB'), ('work', 'NN'), (\"'ve\", 'VBP'), ('many', 'JJ'), ('different', 'JJ'), ('symptoms', 'NNS'), ('new', 'JJ'), ('one', 'CD'), ('My', 'NNP'), ('breast', 'NN'), ('really', 'RB'), ('hurt', 'VB'), ('Both', 'DT'), ('sides', 'NNS'), ('hurt', 'VBP'), ('hard', 'JJ'), ('touch', 'NN'), ('making', 'VBG'), ('appointment', 'JJ'), ('doctor', 'NN'), ('Has', 'NNP'), ('anyone', 'NN'), ('else', 'RB'), ('happen', 'VB'), ('truly', 'RB'), ('want', 'JJ'), ('feel', 'NN'), ('better', 'RBR')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDp4L3GO2OE0",
        "outputId": "e408a031-1867-49d3-963b-c8b7507b5c3c"
      },
      "source": [
        "posTagList = [\"NN\", \"NNS\", \"NNP\", \"VB\", \"VBG\"]\n",
        "for i in range(len(out)-1):\n",
        "  if ((nltk.pos_tag(last[0][i])[0][1]) not in posTagList):\n",
        "    print(nltk.pos_tag(last[0][i]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('covid', 'JJ'), ('September', 'NNP')]\n",
            "[('9', 'CD'), ('days', 'NNS')]\n",
            "[('2', 'CD'), ('months', 'NNS')]\n",
            "[('back', 'RB'), ('work', 'NN')]\n",
            "[(\"'ve\", 'VBP'), ('many', 'JJ')]\n",
            "[('many', 'JJ'), ('different', 'JJ')]\n",
            "[('different', 'JJ'), ('symptoms', 'NNS')]\n",
            "[('new', 'JJ'), ('one', 'CD')]\n",
            "[('one', 'CD'), ('My', 'PRP$')]\n",
            "[('My', 'PRP$'), ('breast', 'NN')]\n",
            "[('really', 'RB'), ('hurt', 'VB')]\n",
            "[('hurt', 'VBN'), ('Both', 'DT')]\n",
            "[('Both', 'DT'), ('sides', 'NNS')]\n",
            "[('hurt', 'VBN'), ('hard', 'JJ')]\n",
            "[('hard', 'RB'), ('touch', 'NN')]\n",
            "[('else', 'RB'), ('happen', 'VB')]\n",
            "[('truly', 'RB'), ('want', 'VBP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4c0J0jbzfMK",
        "outputId": "5bb74f04-ee98-4d35-ff1d-1384a1f1f73c"
      },
      "source": [
        "\n",
        "words1 = set(nltk.corpus.words.words())\n",
        "words1 = [word for word in out if word.lower() in words1]\n",
        "print(words1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['covid', 'spent', 'days', 'hospital', 'home', 'going', 'back', 'work', 'many', 'different', 'new', 'one', 'My', 'breast', 'really', 'hurt', 'Both', 'sides', 'hurt', 'hard', 'touch', 'making', 'appointment', 'doctor', 'anyone', 'else', 'happen', 'truly', 'want', 'feel', 'better']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jqiYgOe0T3z",
        "outputId": "1070202d-b530-4fbc-e290-d6b1dadd4377"
      },
      "source": [
        "posTagList = [\"NN\", \"NNS\", \"NNP\", \"VB\", \"VBG\"]\n",
        "last = [pd.Series(nltk.ngrams(words1, 2))]\n",
        "for i in range(len(words1)-2):\n",
        "  if ((nltk.pos_tag(last[0][i])[1][1]) == \"NN\" and (nltk.pos_tag(last[0][i])[0][1]) == \"NN\"):\n",
        "    print(nltk.pos_tag(last[0][i]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('covid', 'NN'), ('spent', 'NN')]\n",
            "[('days', 'NNS'), ('hospital', 'NN')]\n",
            "[('hospital', 'NN'), ('home', 'NN')]\n",
            "[('back', 'RB'), ('work', 'NN')]\n",
            "[('My', 'PRP$'), ('breast', 'NN')]\n",
            "[('hard', 'RB'), ('touch', 'NN')]\n",
            "[('touch', 'NN'), ('making', 'NN')]\n",
            "[('making', 'VBG'), ('appointment', 'NN')]\n",
            "[('appointment', 'NN'), ('doctor', 'NN')]\n",
            "[('doctor', 'NN'), ('anyone', 'NN')]\n",
            "[('happen', 'VB'), ('truly', 'NN')]\n",
            "[('want', 'NN'), ('feel', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVCT6dQY2Bit"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "txt = \"Anyone elseHave abdominals issues, just aches kinda all over at times, the pain seems to Move around . It‚ÄôsNot excruciating but a Dull ache and congestion still comesAnd goes. So depressing!\"\n",
        "text_tokens = word_tokenize(txt)\n",
        "text_tokens = [word for word in text_tokens if word.lower() in text_tokens]\n",
        "doc = nlp(\"Anyone elseHave abdominals issues, just aches kinda all over at times, the pain seems to Move around . It‚ÄôsNot excruciating but a Dull ache and congestion still comesAnd goes. So depressing!\")\n",
        "# for token in doc:\n",
        "#   print(token.text, token.pos_, token.tag_)\n",
        "# NOUN NOUN, NOUN ADV VERB, after AUX NN, ADJ NOUN NOUN, ADJ NOUN\n",
        "# After AUX ADV ADJ CCONJ ADJ, SCONJ NOUN\n",
        "bigram = [pd.Series(nltk.ngrams(text_tokens, 2))]\n",
        "trigram = [pd.Series(nltk.ngrams(text_tokens, 3))]\n",
        "# print(last)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXJJQ6sTz6X8",
        "outputId": "702acaa1-7587-43b2-df70-bf9c96cb2365"
      },
      "source": [
        "\n",
        "for i in range(len(text_tokens)-2):\n",
        "  # print(nltk.pos_tag(last[0][i]))\n",
        "  bipattern = [\"JJ\"+\"NN\", \"NN\"+\"NN\", \"NNS\"+\"NNS\", \"AUX\"+\"NN\", \"CC\"+\"NN\", \"NNP\"+\"NN\"]\n",
        "  tripattern = [\"NN\"+\"RB\"+\"VBZ\"]\n",
        "  if ((nltk.pos_tag(bigram[0][i])[0][1])+(nltk.pos_tag(bigram[0][i])[1][1])) in bipattern:\n",
        "    print(nltk.pos_tag(bigram[0][i]))\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('abdominals', 'NNS'), ('issues', 'NNS')]\n",
            "[('and', 'CC'), ('congestion', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNG4QZY78B1_",
        "outputId": "455a7d31-ddd6-4e18-af43-8a27fa365390"
      },
      "source": [
        "bigramtoken = [pd.Series(nltk.ngrams(doc, 2))]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx5HsrNf3jVn",
        "outputId": "75ee22f6-f3d9-4084-e516-5a31a733f32e"
      },
      "source": [
        "\n",
        "for token in doc:\n",
        "  print(token.text, token.pos_, token.tag_)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anyone PRON NN\n",
            "elseHave PUNCT HYPH\n",
            "abdominals NOUN NNS\n",
            "issues NOUN NNS\n",
            ", PUNCT ,\n",
            "just ADV RB\n",
            "aches VERB VBZ\n",
            "kinda ADV RB\n",
            "all ADV RB\n",
            "over ADV RB\n",
            "at ADP IN\n",
            "times NOUN NNS\n",
            ", PUNCT ,\n",
            "the DET DT\n",
            "pain NOUN NN\n",
            "seems VERB VBZ\n",
            "to PART TO\n",
            "Move VERB VB\n",
            "around ADV RB\n",
            ". PUNCT .\n",
            "It‚ÄôsNot ADV RB\n",
            "excruciating ADJ JJ\n",
            "but CCONJ CC\n",
            "a DET DT\n",
            "Dull PROPN NNP\n",
            "ache NOUN NN\n",
            "and CCONJ CC\n",
            "congestion NOUN NN\n",
            "still ADV RB\n",
            "comesAnd PROPN NNP\n",
            "goes VERB VBZ\n",
            ". PUNCT .\n",
            "So ADV RB\n",
            "depressing ADJ JJ\n",
            "! PUNCT .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKxvTg2qwaNB"
      },
      "source": [
        "import textacy\n",
        "about_talk_text = ('The talk will introduce reader about Use cases of Natural Language Processing in Fintech')\n",
        "pattern = r'(<NOUN>?<ADV>*<VERB>+)'\n",
        "about_talk_doc = textacy.make_spacy_doc(about_talk_text, lang='en_core_web_sm')\n",
        "verb_phrases = textacy.extract.pos_regex_matches(about_talk_doc, pattern)\n",
        "# Print all Verb Phrase\n",
        "for chunk in verb_phrases:\n",
        "  print(chunk.text)\n",
        "\n",
        "# Extract Noun Phrase to explain what nouns are involved\n",
        "for chunk in about_talk_doc.noun_chunks:\n",
        "  print (chunk)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}